---
layout:     post
title:      "删除不在数据库中的s3文件"
subtitle:   ""
date:       2024-09-08
author:     " Shallow Dreamer"
header-img: "img/post-bg-js-version.jpg"
tags:
    - s3
    - django
---

为了提升性能，可以通过以下几种优化方式来加快清理不再被引用的S3文件的过程，特别是对于大规模文件操作和复杂数据库遍历的场景：

### 性能优化方案：

1. **批量查询数据库**：
   - 避免一次性加载所有数据，而是分批次从数据库中获取数据，减少内存压力。例如，使用Django的`iterator()`方法来节省内存。
   
   ```python
   # 批量迭代获取数据库中的附件信息
   def get_current_files_in_batches(batch_size=1000):
       current_files = set()
       for record in YourModel.objects.iterator(chunk_size=batch_size):
           json_data = json.loads(record.json_field)
           current_files.update(json_data.get('attachments', []))
       return current_files
   ```

2. **异步并行处理**：
   - 使用异步任务处理S3文件的删除操作，可以通过Python的`concurrent.futures`或`celery`来并发处理文件的删除，以加快删除速度。

   使用`concurrent.futures`示例：

   ```python
   import boto3
   from concurrent.futures import ThreadPoolExecutor
   
   s3_client = boto3.client('s3')
   bucket_name = 'your-bucket-name'
   
   def delete_file(file_name):
       s3_client.delete_object(Bucket=bucket_name, Key=file_name)
       print(f"Deleted {file_name} from S3")
   
   def delete_s3_files_in_parallel(files_to_delete):
       with ThreadPoolExecutor(max_workers=10) as executor:  # 使用10个并发线程
           executor.map(delete_file, files_to_delete)
   ```

3. **批量删除S3文件**：
   - S3支持批量删除最多1000个文件，可以一次性删除多个文件，这将显著提升删除效率。

   ```python
   def delete_s3_files_in_bulk(files_to_delete):
       s3_client = boto3.client('s3')
       bucket_name = 'your-bucket-name'
       objects = [{'Key': file_name} for file_name in files_to_delete]
   
       # S3 批量删除操作
       s3_client.delete_objects(Bucket=bucket_name, Delete={'Objects': objects})
   ```

4. **减少数据库查询的开销**：
   - 将所有查询操作合并到一个操作中，以减少数据库交互的次数。可以直接查询所有JSON字段中包含的附件，并利用`JSONB`字段的索引提高查询速度。
   - 在Postgres中，可以使用`jsonb_array_elements`函数来直接从数据库的JSON字段中提取数组元素：

   ```sql
   SELECT jsonb_array_elements(json_field->'attachments') AS file_name
   FROM your_model;
   ```

5. **缓存机制**：
   - 如果文件列表变化不频繁，可以将数据库中已引用的文件信息缓存到内存或Redis中，避免每次都从数据库中重新查询，尤其是当附件数据量较大时。

6. **合理的调度频率**：
   - 根据业务需求调整清理任务的调度频率。频繁地遍历和清理可能会增加系统负载，建议根据文件的增长速度和删除需求设定合理的调度间隔。

7. **分页获取S3文件**：
   - 如果S3中存储的文件很多，可以分页获取文件来避免一次性加载大量文件，减少API调用和内存消耗。

   ```python
   def get_all_s3_files():
       s3_client = boto3.client('s3')
       bucket_name = 'your-bucket-name'
       s3_files = set()
       paginator = s3_client.get_paginator('list_objects_v2')
   
       for page in paginator.paginate(Bucket=bucket_name):
           for content in page.get('Contents', []):
               s3_files.add(content['Key'])
       
       return s3_files
   ```

### 优化后的流程：

1. **批量遍历数据库**，每次获取一定量的附件数据，减少内存占用。
2. **并行处理S3删除操作**，利用多线程加快文件删除速度。
3. **批量删除S3文件**，通过S3批量删除API每次删除最多1000个文件，提升效率。
4. **减少数据库查询**，通过优化查询语句和索引，提升数据提取速度。

这种方式可以确保系统具备较好的性能，即便在大量数据和文件操作的场景下，也能够保持高效的执行速度。

